---
title: "Assignment 1"
author: "Michaela Lukacova"
date: "`r Sys.Date()`"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

library(ggplot2)
library(MASS)
library(gridExtra)
library(data.table)
library(lme4)
theme_set(theme_bw())
```

# Part 1

## 5.

We simulate the data. We simulate $10^5$ data points form the multivariate normal distribution with mean $\begin{pmatrix} 0 \\ 0 \\ 0 \end{pmatrix}$ and variance $\begin{pmatrix} 1 & 0.25 & 0.5 \\ 0.25 & 1 & 0.5 \\ 0.5 & 0.5 & 1 \end{pmatrix}$. 

```{r}
set.seed(7878)
n <- 10^5
sigma <- matrix(c(1, 0.25, 0.5, 0.25, 1, 0.5, 0.5, 0.5, 1), nrow = 3)
X <- mvrnorm(n, c(0,0, 0), sigma)
```

In order to condition on $|X_3| = 0$ we find the rows of $X$ for which $|X_3| \leq 0.05$:

```{r}
cond <- which(abs(X[,3]) < 0.05)
```

We calculate the correlation of $X_1$ and $X_2$ for those $X_1$ and $X_2$ for which $|X_3| \leq 0.05$. 

```{r}
cor(X[cond,1], X[cond,2])
```

Which is as expected close to $0$. This is different from the correlation of $X_1$ and $X_2$ in general which is by construction $\approx 0.25$. 

```{r}
cor(X[,1], X[,2])
```

To visualize the conditional distribution of $X_1$ and $X_2$ we create the two scatterplot below. In the unconditional distribution the two variables are positively correlated, while they in the conditional distribution are uncorrelated, and thus (since they are normal) independent. 

```{r, fig.width=10, fig.height=4, fig.align='center'}
grid.arrange(
ggplot()+
  geom_point(aes(x = X[cond,1], y = X[cond,2]), alpha = 0.2, col = "steelblue")+
  labs(x = "X1", y = "X2", title = "Conditional on |X3| < 0.1 "),
ggplot()+
  geom_point(aes(x = X[1:10000,1], y = X[1:10000,2]), alpha = 0.2, col = "steelblue")+
  labs(x = "X1", y = "X2", title = "Unconditional"),
nrow = 1)
```
As a last illustration, we have made histograms of the marginal distributions of the two random variables, conditional on $X_3 = 0$ and unconditionally. 

```{r}
d_x <- function(x) dnorm(x, mean = 0, sd = 1)
d_x_cond <- function(x) dnorm(x, mean = 0, sd = sqrt(0.75))

grid.arrange(
ggplot()+
  geom_histogram(aes(x = X[cond,1], y = ..density.., fill = "Conditional"), alpha = 0.3)+
  geom_histogram(aes(x = X[1:10000,1], y = ..density.., fill = "Unconditional"), alpha = 0.3)+
  geom_function(fun = d_x, aes(col = "Unconditional"))+
  geom_function(fun = d_x_cond, aes(col = "Conditional"))+
  scale_color_manual(values = c("Conditional" = "steelblue","Unconditional" = "hotpink"))+
  labs(x = "X1", y = "density", title = "Histogram of X1"),
ggplot()+
  geom_histogram(aes(x = X[cond,2], y = ..density.., fill = "Conditional"), alpha = 0.3)+
  geom_histogram(aes(x = X[1:10000,2], y = ..density.., fill = "Unconditional"), alpha = 0.3)+
  geom_function(fun = d_x, aes(col = "Unconditional"))+
  geom_function(fun = d_x_cond, aes(col = "Conditional"))+
  scale_color_manual(values = c("Conditional" = "steelblue","Unconditional" = "hotpink"))+
  labs(x = "X2", y = "density", title = "Histogram of X2"),
nrow = 1)
```

# Part 2

```{r}
load("assignment2024-1.Rdata")

head(likingdata)
summary(likingdata)
```

Data plots:

```{r}
likingdata <- data.table(likingdata)

ggplot(data = likingdata)+
  geom_boxplot(aes(x = Product, y = Liking))
```

Model fits:

```{r}
fit1 <- lmer(Liking ~ Product + (1|Participant) + (1|Class), data=likingdata)
fit2 <- lmer(Liking ~ ProdVersion + ProdType + (1|Participant) + (1|Class), data=likingdata)
```

## 1.

The statistical model from fit1, can be described as:

$$Y_{ijk} = \beta_{i} + B_{j}^{\text{par}} + B_{k}^{\text{class}} + \epsilon_{ijk}$$ 

Where we use triple indexing. $i=1,\ldots, 6$ corresponds to the $6$ different products, $j=1,\ldots,75$ corresponds to the $75$ different participants, $k=1,\ldots,5$ corresponds to the $5$ different classes.

Where $B^{\text{par}} \sim \mathcal{N}_{75}(0, \tau_{\text{par}}^2 I_{75})$, $B^{\text{class}} \sim \mathcal{N}_{5}(0, \tau_{\text{class}}^2 I_{5})$ and $\epsilon \sim \mathcal{N}_{450}(0, \sigma^2 I_{450})$ are all independent. 

Participant and class are considered random effects, since we are not interested in the specific participants or classes as such, but rather as representatives of the population of children and classes. 

Product is included as a fixed effect since we are interested in investigating the child friendliness of the different products. 

## 2.

## 3. 

The factor `Product` is the interaction of the two factors `ProdVersion` and `ProdType`, therefore the subspace spanned by `ProdVersion` and `ProdType` is included in the subspace spanned by `Product`, and fit2 is thus a submodel of fit1. In other words if we know the `Product` we also know the `ProdVersion` and `ProdType`.

In fit1 we estimate $6$ different fixed effects parameters, 1 intercept parameter, and then $5$ additional parameters for each additional interaction level between `ProdVersion` and `ProdType`. In fit2 we estimate $4$ fixed effect parameters, $1$ intercept parameter, $2$ for each additional `ProdVersion` level and $1$ for the last level of `ProdType`. We assume no interaction effects between the two factors `ProdVersion` and `ProdType` in fit2. 

Letting $L_0$ denote the subspace of $\RR^n$ spanned by the model matrix from fit2 and $L_X$ denote the subspace of $\RR^n$ spanned by the model matrix from fit1, we can test the hypothesis of $EY \in L_0 \subseteq L_X$ with the likelihood ratio statistic. The test relies on asymptotic results which we use without further arguments. We perform the test by use of the anova command: 

```{r}
anova(fit1,fit2)
```
With a significance level of 0.05 we can most certainly not reject the null hypothesis, and we can conclude that the interaction factor `product` does not improve the model fit significantly. 

*Jeg tænker at siden at p-værdien er så høj at vi ikke gider at simulere?*

```{r}
## Simulated p-value in test for TVset
#sim12 <- pbkrtest::PBmodcomp(fit1, fit2, nsim=2000, seed=967)
#
## Extract simulated LRTs
#LRT_12 <- as.numeric(sim12$ref)
#
## Density for chi-square with df=1
#dchisq2 <- function(x) dchisq(x,df=2)
#
## Histogram with overlaid density
#data.frame(LRT_12 = LRT_12) |> 
#  ggplot(aes(x = LRT_12)) + 
#  geom_histogram(aes(y = ..density..), breaks=seq(0,18,0.5), color="black", fill="white") +
#  geom_function(fun = dchisq2, colour = "red", xlim=c(0.12,15), linewidth=1) +
#  xlab("LRT") + ylab("Density") + ggtitle("Test for ProdVersion effect") +
#  geom_vline(xintercept=0.7031, color="blue",linewidth=1, linetype="dashed")
```

```{r}
#ggplot()+
#  geom_histogram(aes(x = (1 - pchisq(LRT_12, df = 2)), y = ..density..))+
#  geom_hline(yintercept = 1)
```

## 4. 

## 5. 

## 6.

### Simulating from the t-distribution

The variance of the $t$-distribution with $\nu$ degrees of freedom is:

$$\frac{\nu}{\nu-2} = \frac{3}{3-2} = 3$$
In order to achieve a variance of $\sigma^2$ we would therefore need to scale $X \sim t(3)$ with

$$\sigma^2= V(c \cdot X) = c^2 3 \Leftrightarrow c = \sqrt{\frac{\sigma^2}{3}}$$

We define the scaling factors

```{r}
c_par <- tauP / sqrt(3)
c_class <- tauC / sqrt(3)
c_eps <- sigma / sqrt(3)
```


```{r, message = FALSE}
deltasim2 <- matrix(NA,M,3)

for (i in 1:M){
  B1 <- rt(n = n_par, df = 3) * c_par
  B2 <- rt(n = n_class, df = 3) * c_class
  eps <- rt(n = n_eps, df = 3) * c_eps
  B <- c(B1,B2)
  y <- X %*% beta + Z %*% B + eps
  y <- y |> as.numeric() # NB. This seems to be necessary
  lmm2 <- lmer(y ~ ProdVersion + ProdType + (1|Participant) + (1|Class), data=likingdata)
  deltasim2[i,1] <- fixef(lmm2)[4]
  deltasim2[i,2:3] <- (lmm2 |> confint(method="Wald"))[7,]
}

deltasim2 <- deltasim2 |> data.frame()
names(deltasim2) <- c("est","lower","upper")
```

Bias:

```{r}
mean(deltasim2$est - fixef(fit2)[4])
```

Coverage:

```{r}
mean(deltasim2$lower <= fixef(fit2)[4] & fixef(fit2)[4] <= deltasim2$upper)
```

So even if the distribution we draw from is not normal, the estimates are unbiased and confidence intervals obtain coverage.

### Simulating from the exponential distribution

The mean of an exponentially distributed random variable $X$ with rate equal to $1$, is $E(X) = 1$. And the variance of an is $\frac{1}{\lambda^2} = 1 $.  In order to achieve a mean of $0$ and  a variance of $\sigma^2$ we would therefore need to shift and scale $X \sim exp(1)$ with

$$\sigma^2= V(c \cdot X - k) = c^2  \Leftrightarrow c = \sigma$$
and

$$0 = E(\sigma X - k) = \sigma - k \Leftrightarrow k = \sigma$$

We define the shift and scaling constants

```{r}
c_par <- tauP
c_class <- tauC 
c_eps <- sigma
```


```{r, message = FALSE}
deltasim3 <- matrix(NA,M,3)

for (i in 1:M){
  B1 <- rexp(n = n_par, rate = 1) * c_par - c_par
  B2 <- rexp(n = n_class, rate = 1) * c_class -  c_class
  eps <- rexp(n = n_eps, rate = 1) * c_eps - c_eps
  B <- c(B1,B2)
  y <- X %*% beta + Z %*% B + eps
  y <- y |> as.numeric() # NB. This seems to be necessary
  lmm2 <- lmer(y ~ ProdVersion + ProdType + (1|Participant) + (1|Class), data=likingdata)
  deltasim3[i,1] <- fixef(lmm2)[4]
  deltasim3[i,2:3] <- (lmm2 |> confint(method="Wald"))[7,]
}

deltasim3 <- deltasim3 |> data.frame()
names(deltasim3) <- c("est","lower","upper")
```

Bias:

```{r}
mean(deltasim3$est - fixef(fit2)[4])
```

Coverage:

```{r}
mean(deltasim3$lower <= fixef(fit2)[4] & fixef(fit2)[4] <= deltasim3$upper)
```

Also okay. 

## 7.

We first examine the residuals. We do this by plotting the fitted values at level one against the residuals. 

```{r}
ggplot(data = data.frame(fitted(fit2)), aes(x = fitted(fit2), y = residuals(fit2)))+
  geom_point()+
  geom_smooth(se = FALSE)
```
The lines are a result of the the response being discrete. The lines make it seem as if there are trends in the residuals, but as the geom_smooth shows, the residuals have approximately mean $0$.

QQ-plot:

```{r}
fit2 |> resid() |> qqnorm(main="")
```
Looks good. We now turn to inspect the predicted random effects.  They look OK. 

```{r}
library(lattice)
fit2 |> ranef() |> dotplot()
qqmath(ranef(fit2)$Participant[,1], main="Participant")
qqmath(ranef(fit2)$Class[,1], main="Class")
```

Maybe some simulation to conclude whether the plots looks like they should.

## 8.

```{r, message = FALSE}
deltasim4 <- matrix(NA,M,3)

for (i in 1:M){
  y <- simulate(fit2)$sim_1 |> round()
  lmm2 <- lmer(y ~ ProdVersion + ProdType + (1|Participant) + (1|Class), data=likingdata)
  deltasim4[i,1] <- fixef(lmm2)[4]
  deltasim4[i,2:3] <- (lmm2 |> confint(method="Wald"))[7,]
}

deltasim4 <- deltasim4 |> data.frame()
names(deltasim4) <- c("est","lower","upper")
```

Bias:

```{r}
mean(deltasim4$est - fixef(fit2)[4])
```

Coverage:

```{r}
mean(deltasim4$lower <= fixef(fit2)[4] & fixef(fit2)[4] <= deltasim4$upper)
```

Evt et histogram i de andre opgaver også. 



