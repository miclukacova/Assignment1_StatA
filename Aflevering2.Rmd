---
title: "Assignment 2"
output:
  pdf_document: default
  html_document: default
---

```{r setup, include=FALSE, warning=FALSE, message=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


```{r, include=FALSE, warning=FALSE, message=FALSE}
library(ggplot2)
library(MASS)
library(gridExtra)
library(tidyverse)
library(nlme)
library(lme4)
library(glmmTMB)
theme_set(theme_bw())
library(rstan)
```

```{r, include=FALSE, warning=FALSE, message=FALSE}
load("assignment2024-2.Rdata")
```

# 1.

We create the `averagedata` by using tidyverse commands, while making sure that other variables are retained whenever it makes sense. 

```{r}
blooddata1 <- blooddata %>%
  group_by(Pair) %>%
  summarise(AveBloodvolume = mean(Bloodvolume))

blooddata2 <- blooddata %>%
  dplyr::select(-c("Bloodvolume", "Pair")) %>%
  unique()

averagedata <- cbind(blooddata1, blooddata2)
```

The factor diagram looks like:

![Factor Diagram](IMG.jpeg)

# 2. 

To give an impression about how blood volume varies over calendar year, between males and females and between elite skiers and control exercisers we make scatter plots stratified on `Group` and `Sex`. 

```{r}
averagedata %>%
  ggplot()+
  geom_point(aes(x=AllocatedMonth, y=AveBloodvolume))+
  facet_grid(~Group + Sex)+
  theme(axis.text.x = element_blank())
```
There are clear differences in the four plots, both in variation and in mean. Most clearly the observations belonging to males have a larger blood volume. They also seem to have a larger variation. But this might also be due to there being less observations for the females. There also seems to be a difference between Elite and Control, where the Elite observations seem to have larger blood volume. It is difficult to assess whether the difference between Control and Elite is equally large for both males and females, that is whether there is an interaction effect between `group` and `sex`. Below plots only stratified on respectively `Group`and `Sex` are made. 

```{r}
averagedata %>%
  ggplot()+
  geom_point(aes(x=AllocatedMonth, y=AveBloodvolume))+
  facet_grid(~Group) +
  theme(axis.text.x = element_blank())
```
Here we again see a slight increase in `AveBloodvolume` between Elite and Control. 

```{r}
averagedata %>%
  ggplot()+
  geom_point(aes(x=AllocatedMonth, y=AveBloodvolume))+
  facet_grid(~Sex) +
  theme(axis.text.x = element_blank())
```
It is very clear from the plot that there is a big difference in AveBloodvolume between males and females. 


# 3.

We initially fit a model with the original average blood volume `AveBloodvolume` as response:

```{r}
mod1 <- lmer(AveBloodvolume ~ Sex + Group + Sex:Group + Laboratory + Weight + (1|Subject), data = averagedata)
summary(mod1)
```
The parameters `Sexmale` and `GroupElite` are estimated to `1324.56` and `987.89` with standard error `338.92` and `610.86` respectively. The interaction term `Sexmale:GroupElite` is estimated to `319.20` with standard error `467.38`. We refrain from testing the significance of the effects at this point, as we carry out tests in a later exercise.

We proceed to fit the log10-transformed `AveBloodvolume` as response:

```{r}
mod2 <- lmer(log10(AveBloodvolume) ~ Sex + Group + Sex:Group + Laboratory + Weight + (1|Subject), data = averagedata)
summary(mod2)
```

The parameters `Sexmale` and `GroupElite` are estimated to `0.110` and `0.088` with standard error `0.025` and `0.045` respectively. The interaction term `Sexmale:GroupElite` is estimated to `-0.036` with standard error `0.034`. Again, we refrain from testing the significance of the effects at this point, as we carry out tests in a later exercise.

As a quick diagnostic of the two models we plot the residuals against the fitted values for both models as well as QQ-plots for both models. First the model with `AveBloodvolume` as response:

```{r}
mod1res <- mod1 %>% residuals()
mod1fitted <- mod1 %>% fitted()

mod1Diag <- data.frame(residuals = mod1res, fitted = mod1fitted)

ggplot(mod1Diag, aes(x = fitted, y = residuals)) + 
  geom_point() + 
  geom_smooth() +
  theme_bw() +
  labs(x = "Fitted values", y = "Residuals")

qqnorm(mod1res)
```

We note, that the residuals tend to have a slightly higher variance for high fitted values compared to small fitted values suggesting heteroschedastic residuals and as a consequence that the model does not capture the mean structure of the data very well. The QQ-plot appears to be reasonable. We make similar plots for the model with `log10(AveBloodvolume)` as response:

```{r}
mod2res <- mod2 %>% residuals()
mod2fitted <- mod2 %>% fitted()

mod2Diag <- data.frame(residuals = mod2res, fitted = mod2fitted)

ggplot(mod2Diag, aes(x = fitted, y = residuals)) + 
  geom_point() + 
  geom_smooth() +
  theme_bw() +
  labs(x = "Fitted values", y = "Residuals")

qqnorm(mod2res)
```

It appears from the above plot, that log-transforming the response solved the problem of heteroschedastic residuals. The residuals appear to be randomly scattered around 0 and we therefore conclude that this model captures the mean structure of the data reasonably well. The QQ-plot looks fine again.

*Note: Hun nævnte til forelæsningen at man transformere når vi har skewed data, men det havde vi ikke vel?*

*The log transformation further more changes the way we model effects from additive to multiplicative. It could be that there was a preference in this regard, but we are not aware of such.*

# 4. 

Note that performing tests regarding the median of expected average blood volume corresponds to performing tests regarding the median of expected log transformed average blood volume. This is true since a significant result regarding the median on log scale, nicely translates to a significant result regarding the median on untransformed scale. *Since the median is equal to the mean in the normal distribution, we can carry on with or tests as per usual.*

To test whether the median of average blood volume between Elite and Control subjects differs between women and men we test the following hypothesis.

$$H_0: \beta_{\text{Sex}\times\text{Group}}=0$$
First we perform a LRT using the $\chi^2$-distribution to compute the p-value:
```{r}
mod_red1 <- lmer(log10(AveBloodvolume) ~ Sex + Group + Laboratory + Weight + (1|Subject), 
             data = averagedata)
anova(mod2, mod_red1)
```
The LRT results in an insignificant p-value of 0.2506. Since the $\chi^2$ distribution is only an assumption that is true asymptotically, we must be careful with the p-value. The p-value is quite large, so we are not likely to get a different result by simulation, nonetheless we, in order to verify our results, compute an empirical p-value by simulating under the null hypothesis. 

```{r, fig.width=10, fig.height=4, fig.align='center', echo = FALSE, warning = FALSE, message = FALSE}
# Simulated p-value in test for TVset
sim1 <- pbkrtest::PBmodcomp(mod2, mod_red1, nsim=2000, seed=967)
sim1

# Extract simulated LRTs
LRT_int <- as.numeric(sim1$ref)

# Density for chi-square with df=1
dchisq1 <- function(x) dchisq(x,df=1)

# Histogram with overlaid density
data.frame(LRT_int = LRT_int) |> 
  ggplot(aes(x = LRT_int)) + 
  geom_histogram(aes(y = ..density..), breaks=seq(0,18,0.5), color="black", fill="white") +
  geom_function(fun = dchisq1, colour = "red", xlim=c(0.12,15), linewidth=1) +
  xlab("LRT") + ylab("Density") + ggtitle("Test for interaction effect") +
  geom_vline(xintercept=1.3197, color="blue",linewidth=1, linetype="dashed")

#  Histogram of transformed LRT values
data.frame(x = 1-pchisq(LRT_int, 1)) |> 
  ggplot(aes(x = x)) + 
  geom_histogram(aes(y = ..density..), color="black", fill="white") +
  geom_hline(yintercept = 1, colour = "red", linetype="dashed") +
  xlab("1 - pchisq(LRT)") + ylab("Density") + ggtitle("Test for interaction effect")
```
The simulations result in a p-value of 0.3013, which is even more insignificant than the one we got using the $\chi^2_1$ assumption. To check the approximation we plot the histogram of the distribution of the simulated LRT together with the density curve of the $\chi^2_1$ distribution, as well as a histogram of a transformation of the simulated LRT values. The transformation performed is $1 - F(LRT)$, and the transformed values should be uniformly distributed if the $\chi^2_1$ approximation is valid. From the plots we conclude, that the approximation is not perfect, but not too bad either.

We can also test the hypothesis by using the Wald test statistic with the normal approximation, resulting in the following p-value:

```{r}
# Summary with Wald test statistics but not p-value
mod2 %>% summary() %>% coefficients()

# p-value from asymptotic N(0,1)
2*pnorm(-1.0569530)
```
The p-value is 0.290533 which is in concordance with the other tests. As with the $\chi^2$ approximation the Wald test statistic will asymptotically under the null follow a normal distribution. One must therefore also be critical of the p-value. And last but not least we perform an approximate F-test using the Satterthwaite's approximation:

```{r}
lmerTest::lmer(AveBloodvolume ~ Sex*Group + Laboratory + Weight + (1|Subject), 
             data = averagedata) |> drop1()
```
The approximate F-test results in a very large p-value of 0.499312, much larger than the other ones. Perhaps the Satterthwaite's approximation is not too good. The test statistic follows an exact F distribution when the design is particularly nice in the way described in the lecture notes.

All tests result in insignificant p-values, and we conclude that we cannot reject the null hypothesis, and thus we do not find evidence that the median  in expected average blood volume between Elite and Control differs between men and women. We continue with the model `mod_red1`. 

To test whether there is an overall difference in median average blood volume between Elite and Control subjects, we test the following hypothesis:

$$H_0: \beta_{Group} = 0$$

In the model `mod_red1`. As before we start out by computing the p-value using the the asymptotic $\chi^2$ approximation:

```{r}
mod_red2 <- lmer(log10(AveBloodvolume) ~ Sex + Laboratory + Weight + (1|Subject), 
             data = averagedata)

anova(mod_red1, mod_red2)
```

The effect is not significant with a significance level of 0.05. We again compute an empirical p-value by simulating under the null. 

```{r, fig.width=10, fig.height=4, fig.align='center', echo = FALSE, warning = FALSE, message = FALSE}
# Simulated p-value in test for Group
sim2 <- pbkrtest::PBmodcomp(mod_red1, mod_red2, nsim=2000, seed=967)
sim2

# Extract simulated LRTs
LRT_group <- as.numeric(sim2$ref)

# Histogram with overlaid density
data.frame(LRT_group = LRT_group) |> 
  ggplot(aes(x = LRT_group)) + 
  geom_histogram(aes(y = ..density..), breaks=seq(0,18,0.5), color="black", fill="white") +
  geom_function(fun = dchisq1, colour = "red", xlim=c(0.12,15), linewidth=1) +
  xlab("LRT") + ylab("Density") + ggtitle("Test for Group effect") +
  geom_vline(xintercept=3.2087, color="blue",linewidth=1, linetype="dashed")

#  Histogram of transformed LRT values
data.frame(x = 1-pchisq(LRT_group, 1)) |> 
  ggplot(aes(x = x)) + 
  geom_histogram(aes(y = ..density..), color="black", fill="white") +
  geom_hline(yintercept = 1, colour = "red", linetype="dashed") +
  xlab("1 - pchisq(LRT)") + ylab("Density") + ggtitle("Test for Group effect")
```

The simualted p-value is  $0.09345$, and both tests thus result in p-values larger than 0.05, and we conclude that we cannot reject the null hypothesis, and thus we do not find evidence that there is an overall difference in expected blood volume between Elite and Control subjects. As before the approximation is not perfect, but not too bad either. 

# 5. 

Tag stilling til REML. 

We fit the model:
```{r}
mod3 <- glmmTMB(log10(AveBloodvolume) ~ Group + Sex + Weight + Laboratory + (1|Subject) + 
          ar1(Time-1|Subject), data=averagedata)
```
We read of the different estimates:
```{r}
VarCorr(mod3)
```

```{r}
tau_u <- 3.9349e-02
sigma_w <- 1.9828e-02
phi <- 0.167
sigma <- summary(mod3)$sigma
```

That is: $\tau_U = 3.9349e-02$, $\sigma_W = 1.9828e-02$, $\phi = 0.167$, $\sigma = 2.1931e-05$.
The formula for the variance of $\log_{10} Y_{ij}$ is:
$$Var(\log_{10} Y_{ij}) = V(U_i) + V(\epsilon_i) + V(W_{ij}) = \tau_U^2 + \sigma^2 + \sigma^2_{W}$$
Where we have used independence. The estimate is
```{r}
var_y <- tau_u^2 + sigma^2 + sigma_w^2
var_y
```
And the standard deviation is:
```{r}
sd_y <- sqrt(var_y)
sd_y
```

# 6.

To compute the correlation $Corr(\log_{10} Y_{i,1}, \log_{10} Y_{i,2})$, we first determine the covariance $Cov(\log_{10} Y_{i,1}, \log_{10} Y_{i,2})$:

$$Cov(\log_{10} Y_{i,1}, \log_{10} Y_{i,2}) = Cov(X_{i1}\beta + U_i + W_{i1} + \epsilon_{i1}, X_{i2} \beta + U_i + W_{i2} + \epsilon_{i2}) = VU_i + Cov(W_{i1}, W_{i2}) = \tau_U^2 + \sigma^2_W \phi^{|t_2-t_1|}$$
Note that most covariance terms evaluate to zero because of independence between the $U_i$'s, $\epsilon_{ij}$'s and $W_{ij}$'s. The correlation is thus:

$$Corr(\log_{10} Y_{i,1}, \log_{10} Y_{i,2}) = \frac{Cov(\log_{10} Y_{i,1}, \log_{10} Y_{i,2})}{\sqrt{V(\log_{10} Y_{i,1}) V(\log_{10} Y_{i,2})}} = \frac{\tau^2_U + \sigma^2_W \phi^{|t_2 - t_1|}}{\tau_U^2 + \sigma^2_W + \sigma^2}$$

Plugging in the estimates from the fitted model gives

```{r}
corr_func <- function(tu_2 = tau_u^2, sigmaW_2 = sigma_w^2, sigma_2 = sigma^2, corr = phi, t1, t2){
  nom <- tu_2 + sigmaW_2 * corr^(abs(t2 - t1))
  denom <- tu_2 + sigmaW_2 + sigma_2
  return(nom/denom)
}

corr_func(t1 = 1, t2 = 2)
```

Similar derivations reveal that 

$$Corr(\log_{10} Y_{i,1}, \log_{10} Y_{i,12}) = \frac{\tau^2_U + \sigma^2_W \phi^{|t_{12} - t_1|}}{\tau_U^2 + \sigma^2_W + \sigma^2}$$
and plugging in the model estimates gives:

```{r}
corr_func(t1 = 1, t2 = 12)
```

We can use a similar argument for the final one:

$$Corr(\log_{10} Y_{i,1}, \log_{10} Y_{i,1/30}) = \frac{\tau^2_U + \sigma^2_W \phi^{|t_{1} - t_{1/30}|}}{\tau_U^2 + \sigma^2_W + \sigma^2}$$
such that

```{r}
corr_func(t1 = 1, t2 = 1 + 1/30)
```

# 7.

We fit the model:

```{r}
mod4 <- glmmTMB(log10(AveBloodvolume) ~ Group + Sex + Weight + Laboratory +
          (Group-1||Subject), dispformula=~Group-1, data=averagedata)
```

We extract estimates corresponding to the variance of the random effects with the command `VarCorr`. 

```{r}
VarCorr(mod4)
```
That is is $\tau_{U,\text{Elite}} = 0.036979$ and $\tau_{U,\text{Control}} = 0.042160$ 
```{r, echo = FALSE}
tau_u_elite <- 0.036979
tau_u_control <- 0.042160
```

To find the estimates of $\sigma_{\text{Elite}}$ and $\sigma_{\text{control}}$ we use the command `fixef`. We remember to exponentiate the estimates since the `glmmTMB`function uses the log link in the dispersion model.

```{r}
exp(fixef(mod4)$disp)
```
To concluude, we estimate that $\sigma_{\text{control}} = 0.01916792$ and $\sigma_{\text{Elite}} = 0.01975819$

```{r, echo = FALSE}
sigma_elite <- exp(fixef(mod4)$disp[2])
sigma_control <- exp(fixef(mod4)$disp[1])
```
We note that there is only a very small difference in these estimates, suggesting that the residual variance in the two groups is quite similar.  

# 8. 

```{r}
mod5 <- glmmTMB(log10(AveBloodvolume) ~ Group + Sex + Weight + Laboratory +
                  ar1(Time-1|Subject) + (Group-1||Subject), 
                dispformula=~Group-1, data=averagedata)

summary(mod5)
```

# 9. 

Based on the AICs from question 8. we choose the model from question 5. We refit the model including the `AllocatedMonth` variable:

```{r}
mod6 <- glmmTMB(log10(AveBloodvolume) ~ Group + Sex + AllocatedMonth + Weight + Laboratory + (1|Subject) + ar1(Time-1|Subject), data=averagedata)
summary(mod6)
```
We then carry out a hypothesis test for the effect of `AllocatedMonth` using the `anova()` command:

```{r}
anova(mod6, mod3)
```

We get a p-value of 0.3721 which imply that we cannot reject the null-hypothesis: That `AllocatedMonth` has no effect which suggests, that it is not important to include in the model.

By printing a summary of the model

```{r}
summary(mod6)
```
we see that the p-value for `GroupElite` is `0.0593` so on a 95% significance level it is not significant. It is worth noting that it is a borderline p-value but the conclusion has not changed from the previous question.

# 10. 

*Assume that the gamma distributions for both Fis and Gijs have the shape parameter equal to the rate parameter. Why is this appropriate?*

The mean of a gamma distributed random variable $U$ with shape $\alpha$ and rate $\lambda$ is given by $EU = \frac{\alpha}{\lambda}$. If we let $\alpha = \lambda$ we get

$$\alpha = \lambda  \Rightarrow EU =  \frac{\alpha}{\lambda} = 1 $$
The model where $F_i$ and $G_{ij}$ both have mean $1$ corresponds to the model where the mean of $U_i$ and $\epsilon_{ij}$ is $0$. This corresponds to the standard assumptions about the mean of the random effects and residuals.

*What is the conditional distribution of $Y_{ij}$ given $F_i$? Check for yourself that this is what is implemented in the Stan code.*

Let the shape and rate parameter of the Gamma distribution of $G_{ij}$ be denoted by $\alpha_2$, then

$$Y_{ij}|F_i = 10^{X_{ij}\beta}F_iG_{ij}|F_i \stackrel{D}{=} \text{Gamma}(\alpha_2, \frac{\alpha_2}{10^{X_{ij}\beta}F_i})$$
Since we condition on $F_i$ the whole term $10^{X_{ij}\beta}F_i$ can be regarded as a constant and the second distributional equality follows from properties of the Gamma distribution. 

In the Stan code we have specified the distribution of $Y_{ij}$ as `gamma(shapeG, shapeG/pow(10,X*beta+Z*U))` which is exactly what we have stated above ($Z$ is the model matrix for the random effects). 

*Which priors are used for (hyper)parameters in the Stan program? Explain why it is certain that the posterior is a proper distribution.*

For $\beta$ we use a $\mathcal{N}(0,10)$, for $shape_F$ and $shape_G$ we use $Gamma(1,1)$, for $F$ we use $Gamma(shape_F, shape_F)$. We are certain that the posterior is a proper distribution as all the priors are proper priors.

# 11.
We first need to make data as a list.

```{r}
# To return number of predictors
fixef_MM <- mod1 %>% model.matrix() 
ranef_MM <- mod1 %>% getME(name="Z") %>% as.matrix() 
```


```{r}
noObs <- nrow(fixef_MM)               # number of obs 
p <- ncol(fixef_MM)                   # number of predictors
noSubj <- ncol(ranef_MM)              # number of subjects 
y <- averagedata$AveBloodvolume       # response, i.e. untransformed average bloodvolume
X <- fixef_MM                         # Model matrix for fixed effects
Z <- ranef_MM                         # Model matrix for random effects
```


```{r}
data_list <- list(noObs = noObs,
                  p = p,
                  noSubj = noSubj,
                  y = y,
                  X = X,
                  Z = Z)
```

```{r}
library(rstan)

regFit <- stan(
  file = "lmm-gamma.stan",  # Stan program
  data = data_list,    # named list of data
  chains = 4,         # number of Markov chains
  warmup = 750,      # number of warm-up iterations per chain
  iter = 3000,        # total number of iterations per chain
  cores = 4,          # number of cores (could use one per chain)
  refresh = 0         # no progress shown
)
```

```{r}
traceplot(regFit)
```

From the traceplots it appear that the markov chains have converged to the same stationary distribution. 

```{r}
regFit
```

```{r}
sim <- rstan::extract(regFit)
names(sim)
hist(sim$beta[,1], main="") 
```


# 12. 

Let $X_{ij,A}, X_{ij,B}$ and $X_{ij,C}$ be the covariate information corresponding to a blood sample belonging to an elite female skier with body weight 80 kg from respectively lab A, B and C. 

```{r}
Xij_a <- c(1, 0, 1, 0, 0, 80, 0)
Xij_b <- c(1, 0, 1, 1, 0, 80, 0)
Xij_c <- c(1, 0, 1, 0, 1, 80, 0)
```

In order to get samples from the posterior distribution for $\mu = E(Y_{ij}|X_{ij}) = 10^{X_{ij} \beta}$ we can use the simulations from the posterior distribution of $\beta$ created by the `stan` command in the previous question. For each draw from the posterior we calculate three values $\mu_A = 10^{X_{i, Aj} \beta}$, $\mu_B = 10^{X_{ij, B} \beta}$ and $\mu_C = 10^{X_{ij, C} \beta}$, corresponding to the expected blood volume of the elite female skier from respectively lab A, B and C. 

```{r}
mu_a <- 10^(as.matrix(sim$beta) %*% Xij_a)
mu_b <- 10^(as.matrix(sim$beta) %*% Xij_b)
mu_c <- 10^(as.matrix(sim$beta) %*% Xij_c)
```

To find the average over the three labs, we for each draw take the average of $\mu_A$, $\mu_B$ and $\mu_C$.

```{r}
EY <- numeric(1400)

for (i in 1:1400){
  EY[i] <- mean(mu_a[i], mu_b[i], mu_c[i])
}
```

Now we can simply take the mean of the draws from the posterior of $\mu$, and obtain the posterior mean of $\mu$ to be:

```{r}
mean(EY) %>% knitr::kable(col.names = " ")
```

By taking the quantiles of the posterior draws we find the posterior interval for $\mu$ to be

```{r}
t(quantile(EY, c(0.025, 0.975))) %>% knitr::kable() 
```

